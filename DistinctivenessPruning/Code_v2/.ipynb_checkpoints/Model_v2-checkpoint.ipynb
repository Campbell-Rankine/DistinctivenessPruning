{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02 - Construct the Neural Network\n",
    "\n",
    "This will be the actual construction, training, testing, validation of the model. We will provide brief justification for each step because it will make writing the actual report easier.\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "    1. Make sure import statements work (DONE!)\n",
    "    2. Return a LOOCV Train Test split.\n",
    "    3. Preprocessing function should output a 2-D tensor. We will construct a 3-D tensor for all values. (NAH)\n",
    "    4. Train and Construct the model. (CONSTRUCTED)\n",
    "    5. Evaluation metrics for base model. (WILL BE INCLUDED)\n",
    "    6. Compare with distinctiveness pruning.\n",
    "    7. If all of this is done on time then add additional EA feature and hyperparameter selection (GREEDY)\n",
    "\n",
    "______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###IMPORTS###\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split #pre all but one training\n",
    "from sklearn.preprocessing import MinMaxScaler #normalize data\n",
    "from sklearn.metrics import confusion_matrix #analysis\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Visualization Imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Machine Learning Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #Neural Network used in Comp4660 at ANU\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - GLOBALS - ###\n",
    "\n",
    "Y = pd.read_csv('subjective-belief_v2/TimeseriesObserver/subjective_belief_observers_ts_labels.csv')\n",
    "cats = ['PUPIL', 'EDA', 'BVP', 'TEMP', 'tags']\n",
    "colors = ['tab:red', 'tab:blue', 'tab:green', 'tab:pink', 'tab:olive', 'tab:orange']\n",
    "num_l_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### - SEARCH FOR FUNCTIONS BASED ON PID_VID - ###\n",
    "def fetch_TS(ID, index, category):\n",
    "    #Define base pathway\n",
    "    bp = 'subjective-belief_v2/TimeseriesObserver/raw_observer_physio_data/'\n",
    "    #fetch outward file\n",
    "    f1=ID[0:3] + '/'\n",
    "    #We know that pairs of video data were gathered. So we must take this one and the ID afterwards\n",
    "    idp1 = Y['pid_vid'][index+1]\n",
    "    pair = ID[4:] + idp1[4:] + '/'\n",
    "    return pd.read_csv(bp + f1 + f1[0:3] + '_' + pair + category + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - RETURN TAGS FROM TAGS.CSV FOR THE DATA - ###\n",
    "def fetch_tags(index):\n",
    "    fv = index.columns[0]\n",
    "    times_array = np.insert(index.to_numpy(), 0, float(fv))\n",
    "    return times_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - FETCH DATA FROM FILES BY INDEX - ###\n",
    "\n",
    "#PLEASE NOTE INDEXES SHOULD INCREASE BY 2! This will also be included in the assignment. As it is necessary.\n",
    "#PreProcessing of features should be conducted before training(I.E. Splitting into local features)\n",
    "def fetch_data(index):\n",
    "    id1 = Y['pid_vid'][index]\n",
    "    Left_Pupil1 = 0 #DEFINE ALL VARIABLES THAT WILL BE RETURNED\n",
    "    Right_Pupil1 = 0\n",
    "    BVP1 = 0\n",
    "    EDA = fetch_TS(Y['pid_vid'][index], index, cats[1])\n",
    "    Temp = fetch_TS(Y['pid_vid'][index], index, cats[3]) #For now we will take the average temperature across the two videos\n",
    "    \n",
    "    #NORMALIZE EDA AND TEMP THEN FIND AVERAGE\n",
    "    sc = MinMaxScaler()\n",
    "    EDA = np.mean(sc.fit_transform(EDA.to_numpy().reshape(-1,1)))\n",
    "    Temp = np.mean(sc.fit_transform(Temp.to_numpy().reshape(-1,1)))\n",
    "    \n",
    "    id2 = Y['pid_vid'][index+1]\n",
    "    Left_Pupil2 = 0 #DEFINE ALL VARIABLES THAT WILL BE RETURNED\n",
    "    Right_Pupil2 = 0\n",
    "    BVP2 = 0\n",
    "    #THESE VALUES WILL BE DATAFRAMES. USED IN THIS DOCUMENT FOR GRAPHING AND ANALYSIS\n",
    "    \n",
    "    #No matter what we will use the later start time to align the data.\n",
    "    tags = fetch_tags(fetch_TS(Y['pid_vid'][index], index, cats[4]))\n",
    "    for x in range(0,len(cats)):\n",
    "        if cats[x] != 'tags':\n",
    "            indexed_point = fetch_TS(Y['pid_vid'][index], index, cats[x])\n",
    "            indices = indexed_point.index.tolist()\n",
    "            ST = float(indexed_point.columns[0]) #Start time for BVP, EDA, TEMP\n",
    "            \n",
    "            si1 = 0\n",
    "            si2 = int((tags[1] - tags[0])) * 60 #start vid 2-start_time / sr\n",
    "            \n",
    "            si1b = 0\n",
    "            si2b = 0\n",
    "            if cats[x] == 'PUPIL':\n",
    "                #Append no preprocessed data size\n",
    "                v1 = 0\n",
    "                v2 = 0\n",
    "                indices1 = []\n",
    "                indices2 = []\n",
    "                if float(tags[0]) > ST:\n",
    "                    v1 = int((float(tags[1]) - float(tags[0]))) * 60\n",
    "                    v2 = int((float(tags[3]) - float(tags[2]))) * 60\n",
    "                else:\n",
    "                    v1 = int((float(tags[1]) - ST)) * 60\n",
    "                    v2 = int((float(tags[3]) - float(tags[2]))) * 60\n",
    "                    si1 = int((ST - tags[0]) * 60)\n",
    "                #FOR THESE NEXT TWO CASES WE FIND THE START INDEX\n",
    "                if v1 > v2: #If video size is different this helps us\n",
    "                    #in this case only use v1 sample #'s'\n",
    "                    indices1 = [int(si1),int((si1+v2))]\n",
    "                    indices2 = [int(si2),int((si2+v2))]\n",
    "                else: \n",
    "                    #in this case only use v2 sample #'s'\n",
    "                    indices1 = [int(si1),int((si1+v1))]\n",
    "                    indices2 = [int(si2),int((si2+v1))]\n",
    "                indices1 = range(indices1[0], indices1[1])\n",
    "                indices2 = range(indices2[0], indices2[1])\n",
    "                #APPEND TO EACH RETURN LIST\n",
    "                Left_Pupil1 = indexed_point.iloc[:,0][list(indices1)]\n",
    "                Right_Pupil1 = indexed_point.iloc[:,0][list(indices1)]\n",
    "                Left_Pupil2 = indexed_point.iloc[:,0][list(indices2)]\n",
    "                Right_Pupil2 = indexed_point.iloc[:,0][list(indices2)]\n",
    "            elif cats[x] == 'BVP':\n",
    "                sr = indexed_point[indexed_point.columns[0]].iloc[0] #Sample rate for measurement\n",
    "                #Since we have sample rate. Should be end-start/samplerate to get all of the indices for that video.\n",
    "                v1 = 0\n",
    "                v2 = 0\n",
    "                indices1 = []\n",
    "                indices2 = []\n",
    "                if float(tags[0]) > ST:\n",
    "                    si1b = int((float(tags[0]) - ST)) #Set to the index of the start of PD being measures\n",
    "                    v1 = (int((float(tags[3]) - float(tags[0]))) * sr) / 2\n",
    "                    v2 = v1\n",
    "                else:\n",
    "                    si1b = int(ST - (float(tags[0]))) * sr\n",
    "                    v1 = int((float(tags[3]) - float(tags[0]))) \n",
    "                    v2 = v1\n",
    "                #FOR THESE NEXT TWO CASES WE FIND THE START INDEX\n",
    "                si2b = (si1b+(v1)) + ((float(tags[2])-float(tags[1])) * sr)\n",
    "                l = v1 * (sr/60)\n",
    "                indices1 = [int(si1b),int((si1b+l))]\n",
    "                indices2 = [int(si2b),int((si2b+l))]\n",
    "                indices1 = range(indices1[0], indices1[1])\n",
    "                indices2 = range(indices2[0], indices2[1])\n",
    "                \n",
    "                #Amend indices1 and 2 so that we can have the same number of sample points\n",
    "                indices1 = list(indices1)\n",
    "                indices2 = list(indices2)\n",
    "                while len(indices1) - Left_Pupil1.shape[0] > 2:\n",
    "                    diff = len(indices1) - Left_Pupil1.shape[0]\n",
    "                    every_X = Left_Pupil1.shape[0] / diff\n",
    "\n",
    "                    del indices1[int(every_X+1)::int(every_X+2)]\n",
    "                    \n",
    "                    del indices2[int(every_X+1)::int(every_X+2)]\n",
    "                \n",
    "                if len(indices1) - Left_Pupil1.shape[0] >= 1:\n",
    "                    diff = len(indices1) - Left_Pupil1.shape[0]\n",
    "                    for i in range(0,diff):\n",
    "                        indices1.pop()\n",
    "                        indices2.pop()\n",
    "                \n",
    "                #APPEND TO EACH RETURN LIST\n",
    "                BVP1 = indexed_point.iloc[:,0][indices1]\n",
    "                BVP2 = indexed_point.iloc[:,0][indices2]\n",
    "    ### - RETURN TWO LISTS 1 FOR EACH DATA POINT. IDENTIFIED BY ID FOR EACH - ###\n",
    "    return [[id1, Left_Pupil1, Right_Pupil1, BVP1, EDA, Temp],\n",
    "                      [id2, Left_Pupil2, Right_Pupil2, BVP2, EDA, Temp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for testing, remove cell once we know preprocess works\n",
    "Example = fetch_data(0)\n",
    "print(Example[0][4], Example[0][5]) #NORMALIZED TEMP AND EDA STATS\n",
    "print(Example[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - DEFINE A PREPROCESSING FUNCTION FOR THE DATA - ###\n",
    "#Note: The input type for the preprocess function will be a list of [id, LP, RP, BVP, EDA (avg), Temp(avg)]\n",
    "#      graphs is simply either 'TRUE' or 'FALSE'\n",
    "#      num_l_features is the number of local features, In how we split the data. Default would trying to analyze the\n",
    "#.     data in 2 second intervals so 16 local features (PD AND BVP).\n",
    "def ts_preprocessLSTM(data, graphs, num_l_features):\n",
    "    #Split and Fetch Data (ease of coding)\n",
    "    LP = np.array(data[1])\n",
    "    RP = np.array(data[2])\n",
    "    BVP = np.array(data[3])\n",
    "    EDA = data[4]\n",
    "    T = data[5]\n",
    "    \n",
    "    #NORMALIZE ALL (EDA AND TEMP ALREADY NORMALIZED)\n",
    "    sc = MinMaxScaler()\n",
    "    LP = sc.fit_transform(LP.reshape(-1,1))\n",
    "    RP = sc.fit_transform(RP.reshape(-1,1))\n",
    "    BVP = sc.fit_transform(BVP.reshape(-1,1))\n",
    "    \n",
    "    #Extract Local Features: This will be used for the Network. We make a list Where Temperature and EDA are just\n",
    "    #                        cloned to match lengths of LP RP and BVP. CHECK THIS\n",
    "    \n",
    "    setLP = []    \n",
    "    setRP = [] \n",
    "    setBVP = []\n",
    "    for x in LP:\n",
    "        setLP.append(x[0])\n",
    "    for x in RP:\n",
    "        setRP.append(x[0])\n",
    "    for x in BVP:\n",
    "        setBVP.append(x[0])\n",
    "        \n",
    "    LP = setLP\n",
    "    RP = setRP\n",
    "    BVP = setBVP\n",
    "    \n",
    "    #Return statement\n",
    "    return [data[0], LP, RP, BVP, EDA, T]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - EXAMPLE OF PREPROCESSED DATA - ###\n",
    "ExamplePP = ts_preprocessLSTM(Example[0], 'TRUE', num_l_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variable for pruning. This can be set to true or false to notice improvements\n",
    "prune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - CONSTRUCT THE NETWORK - ###\n",
    "## - AVAILABLE GPU TRAINING - ##\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "#Calculate number of features total 3*lp/rp/bvp + EDA + Temp\n",
    "num_f = 2\n",
    "print(num_f)\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "print(is_cuda)\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "#Cyclic LR Scheduler\n",
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]\n",
    "\n",
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + np.cos(np.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "#construct network\n",
    "class LSTMTS(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, 1)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.randn(1,1,self.hidden_layer_size),\n",
    "                            torch.randn(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, x, hprev, cprev):\n",
    "        lstm_out, hc = self.lstm(x.view(len(x) ,1, -1), (hprev, cprev))\n",
    "        out = self.fc2(lstm_out.view(len(x), -1))\n",
    "        out = F.sigmoid(out)\n",
    "        #We find unequal lengths in the data so you have to pick the correct final prediction value.\n",
    "        return out, hc\n",
    "\n",
    "#define hyperparameters\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "hidden_dim = 100\n",
    "\n",
    "if not prune:\n",
    "    hidden_dim = 50\n",
    "\n",
    "model = LSTMTS(input_size, hidden_dim, output_size)\n",
    "\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "loss_func = nn.BCELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "sched = CyclicLR(optimiser, cosine(t_max=(304-(304/3)) * 2, eta_min=lr/100))\n",
    "\n",
    "decayRate = lr/30 #30 is num_epochs\n",
    "lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimiser, gamma=decayRate)\n",
    "\n",
    "scheduled = cosine(t_max=(304-(304/5)) * 2, eta_min=lr/100)\n",
    "learning_rates = [scheduled(t, .001) for t in range(int((304-(304/5))) * 4)]\n",
    "plt.plot(learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - CONSTRUCT 3-D Tensor of all data - ###\n",
    "\n",
    "def construct_df(pid_vid):\n",
    "    ### - Should output a df of the test subject - ###\n",
    "    #Start by finding the index of the test subject in Y\n",
    "    i = Y[Y['pid_vid']==pid_vid].index.values[0]\n",
    "    y = Y[Y.columns[1]][i]\n",
    "    i_acc = i\n",
    "    if i % 2 == 1:\n",
    "        i_acc = i-1\n",
    "    retrieve = i_acc-i\n",
    "    sub = fetch_data(i_acc)\n",
    "    column_names = ['lp', 'rp']\n",
    "    #create the list of lists\n",
    "    testsub = ts_preprocessLSTM(sub[retrieve], 'FALSE', num_l_features)\n",
    "    #convert to list of lists w/ correct local features\n",
    "    data = [testsub[1], testsub[2]]\n",
    "    templ = []\n",
    "    edal = []\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.transpose()\n",
    "    df.columns = column_names\n",
    "    return df, y\n",
    "\n",
    "#EXAMPLE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define loss smoothing function\n",
    "smooth_loss = -np.log(1.0/120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Return the dataset as a list of variables in an index ###\n",
    "Xs = []\n",
    "ys = []\n",
    "for i in range(304):\n",
    "    index_id = Y['pid_vid'][i]\n",
    "    X, y = construct_df(index_id)\n",
    "    X = X.dropna()\n",
    "    Xs.append(X)\n",
    "    ys.append(y)\n",
    "Xs = np.array(Xs)\n",
    "ys = np.array(ys)\n",
    "print(len(Xs), len(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for below cell:\n",
    "#define a custom pruning function\n",
    "#calculate angle\n",
    "def calc_angle(x,y):\n",
    "    incos1 = (np.dot(x,y))\n",
    "    incos2 = (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    bet = incos1/incos2\n",
    "    return(math.degrees(math.acos(bet)))\n",
    "def merge_neurons(i, j, module, output_a):\n",
    "    #index of neuron to keep: i; index of neuron to merge: j; module should be the pattern set (net.fc1)\n",
    "    #Retrieve Weights:\n",
    "    with torch.no_grad():\n",
    "        weights = module.state_dict()['weight_hh_l0'].T\n",
    "        torch.transpose(module.state_dict()['weight_hh_l0'], 0, 1)[i] = (module.state_dict()['weight_hh_l0'].T[j] + module.state_dict()['weight_hh_l0'].T[j])/2 #add the two\n",
    "        biases = module.state_dict()['bias_ih_l0']\n",
    "        module.state_dict()['weight_hh_l0'].T[i] = module.state_dict()['weight_hh_l0'].T[i] + module.state_dict()['weight_hh_l0'].T[j] #add the two\n",
    "        module.state_dict()['weight_hh_l0'].T[j] = module.state_dict()['weight_hh_l0'].T[j] * 0 #Remove the j weight\n",
    "        module.state_dict()['bias_ih_l0'][j] = module.state_dict()['bias_ih_l0'][j] * 0 #Remove J bias\n",
    "        #Remove output activation for removed neuron\n",
    "        output_a.weight.data[0][j] = 0 \n",
    "    return True\n",
    "def remove_neurons(i, j, module, output_a):\n",
    "    #index of neurons to remove: i,j; module should be the pattern set (net.fc1)\n",
    "    with torch.no_grad():\n",
    "        module.state_dict()['weight_hh_l0'].T[i] = torch.tensor(0.0, requires_grad=True)\n",
    "        module.state_dict()['bias_ih_l0'][i] = torch.tensor(0.0, requires_grad=True)\n",
    "        module.state_dict()['weight_hh_l0'].T[j] = torch.tensor(0.0, requires_grad=True)\n",
    "        module.state_dict()['bias_ih_l0'][j] = torch.tensor(0.0, requires_grad=True)\n",
    "        output_a.weight.data[0][i] = torch.tensor(0.0, requires_grad=True)\n",
    "        output_a.weight.data[0][j] = torch.tensor(0.0, requires_grad=True)\n",
    "    return True\n",
    "\n",
    "def reset_zeros(pruned_m, pruned_d, module, output_a):\n",
    "    #Resets all the neurons that have been removed. \n",
    "    #retrieve indices to delete\n",
    "    delete = []\n",
    "    for x in pruned_d:\n",
    "        delete.append(x[0])\n",
    "        delete.append(x[1])\n",
    "    for x in pruned_m:\n",
    "        delete.append(x[1])\n",
    "    with torch.no_grad():\n",
    "        #Delete nodes\n",
    "        module.state_dict()['weight_hh_l0'].T[delete] = torch.tensor(0.0, requires_grad=True)\n",
    "        module.state_dict()['bias_ih_l0'][delete] = torch.tensor(0.0, requires_grad=True)\n",
    "        output_a.weight.data[0][delete] = torch.tensor(0.0, requires_grad=True)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruning Cells:\n",
    "#Investigative stage.\n",
    "#Retrieve weights and biases of Input to hidden and hidden to input connections\n",
    "def prune_net():\n",
    "    module1 = model.lstm\n",
    "    module2 = model.fc2\n",
    "\n",
    "    m1params = list(module1.state_dict()['weight_hh_l0'].T)\n",
    "    print(module1.state_dict()['weight_hh_l0'].T.shape)\n",
    "    m2params = list(module2.weight)\n",
    "\n",
    "    #These are just to observe. Was to help me visual and understand the sample space\n",
    "    #print(m1params)\n",
    "    #print(m1params[0].size(0), len(m1params))\n",
    "    #print(m2params)\n",
    "    #print(m2params[0].size(0), len(m2params))\n",
    "\n",
    "    #so we know what we are looking at model1params weight vectors\n",
    "    tmerge = [] #nodes to remove. \n",
    "    trem = []\n",
    "    inspected = []\n",
    "    largest_angle = [0,0,0,0]\n",
    "    smallest_angle = [0,0,180,1]\n",
    "    for i, x in enumerate(m1params):\n",
    "        #X is a tensor we should retrieve the actual vector\n",
    "        vector_x = x.tolist() #convert so we can perform matrix math\n",
    "        #retrieve list of comparison vectors. everything in m1params except vector x\n",
    "        for j, y in enumerate(m1params):\n",
    "            cur = (i,j)\n",
    "            if not i == j and cur not in inspected:\n",
    "                #retrieve Y vector:\n",
    "                vector_y = y.tolist()\n",
    "                angle = calc_angle(vector_x, vector_y)\n",
    "                inspected.append((i,j)) #make sure that the same pair of vectors is not looked at again.\n",
    "                inspected.append((j,i))\n",
    "                if angle > largest_angle[2]:\n",
    "                    largest_angle = [i,j,angle,0]\n",
    "                if angle < smallest_angle[2]:\n",
    "                    smallest_angle = [i,j,angle,1]\n",
    "    print(largest_angle, smallest_angle)\n",
    "    #decide what to prune\n",
    "    difflarge = largest_angle[2] - 125\n",
    "    diffsmall = 55-smallest_angle[2]\n",
    "    if difflarge > diffsmall and largest_angle[2]>=125:\n",
    "        #delete the two counteractive nodes\n",
    "        #print(\"Nodes to remove: \" + str(largest_angle[0]) + \", \" + str(largest_angle[1]) + \", Angle: \" + str(largest_angle[2]) + \" degrees\")\n",
    "        if remove_neurons(largest_angle[0], largest_angle[1], model.lstm, model.fc2):\n",
    "            return largest_angle\n",
    "    elif smallest_angle[2]<=55:\n",
    "        #Merge similar nodes\n",
    "        #print(\"Node to keep: \" + str(smallest_angle[0]) + \", Node to merge: \" \n",
    "        #      + str(smallest_angle[1]) + \", Angle: \" + str(smallest_angle[2]) + \" degrees\")\n",
    "        if merge_neurons(smallest_angle[0], smallest_angle[1], model.lstm, model.fc2):\n",
    "            return smallest_angle\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### - TRAIN THE MODEL LOOCV TRAINING - ###\n",
    "#P29-36 will be ignored#\n",
    "\n",
    "all_losses_e = []\n",
    "all_acc_e = []\n",
    "\n",
    "all_tloss_e = []\n",
    "all_tacc_e = []\n",
    "\n",
    "#Train With Network Pruning By Weights\n",
    "# store all losses for visualisation\n",
    "pruned_neurons_D = [] #allows us to update and set = to 0 every training cycle.\n",
    "pruned_neurons_M = [] #allows us to update and set = to 0 every training cycle.\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "folds = 3\n",
    "\n",
    "#'p29', 'p30', 'p31', 'p32', 'p33', 'p34', 'p35', 'p36' are untrainable.\n",
    "\n",
    "d = list(range(304))\n",
    "shuffle(d) #Data is shuffled\n",
    "\n",
    "d = np.array_split(d, folds+1)\n",
    "\n",
    "val = d[folds] #These are indices\n",
    "\n",
    "hprev = torch.zeros(1,1,hidden_dim)\n",
    "cprev = torch.zeros(1,1,hidden_dim)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Perform forward pass: compute predicted y by passing x to the model.\n",
    "    # Here we pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    #Training Measurements\n",
    "    losses = [] #All losses across all training cycles. To be averaged out for final loss per epoch\n",
    "    acc = []\n",
    "    \n",
    "    losses_t = [] #All losses across testing cycle. To be averaged out for final loss per epoch\n",
    "    correct_t = 0\n",
    "    total_t = 0\n",
    "    \n",
    "    acc_t = []\n",
    "    \n",
    "    correct_train = 0\n",
    "    \n",
    "    #3-Fold CV This is to keep everything pretty small and raise the test set size\n",
    "    for i in range(folds): #304 is when pupillary data stops\n",
    "        output = [] #output from sigmoid values\n",
    "        output_t = []\n",
    "        \n",
    "        curr_test_x = Xs[d[i]]\n",
    "        curr_test_y = ys[d[i]]\n",
    "        \n",
    "        train_i = np.array(d[:i] + d[i+1:])\n",
    "        train_i = np.append(train_i[0], train_i[1])\n",
    "        \n",
    "        curr_train_x = Xs[train_i]\n",
    "        curr_train_y = ys[train_i]\n",
    "        #curr_train_x = Variable(torch.from_numpy(np.array(curr_train_x).astype(np.double)))\n",
    "        \n",
    "        for k, x in enumerate(curr_train_x):\n",
    "            x = Variable(torch.from_numpy(np.array(x).astype(np.float)))\n",
    "            \n",
    "            y = Variable(torch.from_numpy(np.array(curr_train_y[k]).astype(np.int)))\n",
    "            y = y.unsqueeze(-1)\n",
    "            y = y.float()\n",
    "            \n",
    "            #Model train inputs and set\n",
    "            y_pred_train, hcprev = model(x.float(), hprev, cprev)\n",
    "            hprev = hcprev[0].detach()\n",
    "            cprev = hcprev[1].detach()\n",
    "            \n",
    "            loss = loss_func(y_pred_train[-1], y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            y_pred_train = round(y_pred_train[-1].item())\n",
    "            \n",
    "            output.append(y_pred_train)\n",
    "            \n",
    "            sched.step()\n",
    "\n",
    "            #clear gradients\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Perform backward pass: compute gradients of the loss with respect to\n",
    "            # all the learnable parameters of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimiser makes an update to its\n",
    "            # parameters\n",
    "            optimiser.step()\n",
    "                \n",
    "        #Retrieve evaluation metrics\n",
    "        corr = 0\n",
    "        for j, x in enumerate(output):\n",
    "            if x == curr_train_y[j]:\n",
    "                corr = corr + 1\n",
    "        assert len(curr_train_x) == len(output)\n",
    "        \n",
    "        ac = corr/(len(output))\n",
    "        \n",
    "        assert len(curr_train_x) == len(output)\n",
    "        \n",
    "        acc.append(ac)\n",
    "        \n",
    "        ac = 0\n",
    "        \n",
    "        correct_t = 0\n",
    "        \n",
    "        for l, x in enumerate(curr_test_x):\n",
    "            x = Variable(torch.from_numpy(np.array(x).astype(np.float)))\n",
    "            pred_t, hc = model(x.float(), hprev, cprev)\n",
    "            y = Variable(torch.from_numpy(np.array(curr_train_y[l]).astype(np.int)))\n",
    "            y = y.unsqueeze(-1)\n",
    "            y = y.float()\n",
    "            #test loss\n",
    "            t_loss = loss_func(pred_t[-1], y)\n",
    "            losses_t.append(t_loss.item())\n",
    "            output_t.append(pred_t[-1].item())\n",
    "            \n",
    "        for j, x in enumerate(output_t):\n",
    "            if round(x) == curr_test_y[j]:\n",
    "                correct_t = correct_t + 1\n",
    "        \n",
    "        ac_t = correct_t/(len(output_t))\n",
    "        acc_t.append(ac_t)\n",
    "        ac_t = 0\n",
    "        \n",
    "    #Reset all values that are supposed to be deleted to 0\n",
    "    if prune:\n",
    "        ds = '.' * (epoch+1)\n",
    "        print('pruning' + ds)\n",
    "        reset_zeros(pruned_neurons_D, pruned_neurons_M, model.lstm, model.fc2)\n",
    "\n",
    "    if prune:\n",
    "        node = prune_net()\n",
    "        if not node == False:\n",
    "            flag = 0\n",
    "            for x in pruned_neurons_D:\n",
    "                if x[0] == node[0] and x[1] == node[1]:\n",
    "                    flag = 1\n",
    "            for x in pruned_neurons_M:\n",
    "                if x[0] == node[0] and x[1] == node[1]:\n",
    "                    flag = 1\n",
    "            if flag == 0:\n",
    "                if node[3] == 0:\n",
    "                    pruned_neurons_D.append(node)\n",
    "                else:\n",
    "                    pruned_neurons_M.append(node)\n",
    "    ac = np.mean(acc)\n",
    "    ac_t = np.mean(acc_t)\n",
    "    \n",
    "    all_tloss_e.append(np.mean(losses_t))\n",
    "    all_tacc_e.append(ac_t)\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        # convert three-column predicted Y values to one column for \n",
    "\n",
    "        print('Epoch [%d/%d] Loss: %.4f  Accuracy: %.2f %%'\n",
    "                % (epoch + 1, num_epochs, loss.item(), ac*100))\n",
    "        print('Testing Loss: %.4f  TestingAccuracy: %.2f %%'\n",
    "                % (t_loss.item(), ac_t*100))\n",
    "        if prune:\n",
    "            print(pruned_neurons_D, pruned_neurons_M)\n",
    "     \n",
    "    all_losses_e.append(np.mean(losses))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    acc = []\n",
    "    \n",
    "    acc_t = []\n",
    "    \n",
    "    losses_t = []   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph of loss over time(Num Epochs)\n",
    "print((num_epochs), len(all_losses_e))\n",
    "print(all_losses_e)\n",
    "plt.plot(range(0,(num_epochs)), all_losses_e, alpha=0.7, color='tab:red')\n",
    "plt.xlabel('validations')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Graph of Training Loss Over # Epochs')\n",
    "plt.show()\n",
    "\n",
    "print((num_epochs), len(all_losses_e))\n",
    "print(all_tloss_e)\n",
    "plt.plot(range(0,(num_epochs)), all_tloss_e, alpha=0.7, color='tab:blue')\n",
    "plt.xlabel('validations')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Graph of Testing loss Over # Epochs')\n",
    "plt.show()\n",
    "\n",
    "print((num_epochs), len(all_losses_e))\n",
    "print(all_tacc_e)\n",
    "plt.plot(range(0,(num_epochs)), all_tacc_e, alpha=0.7, color='tab:green')\n",
    "plt.xlabel('validations')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Graph of Testing Accuracy Over # Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convusion Matrices and displays for Training and Testing Set\n",
    "import seaborn as sn\n",
    "\n",
    "output_t = np.round(output_t) #This is because raw test numbers were appended to output_t instead of the rounded\n",
    "\n",
    "confusion_train = confusion_matrix(curr_train_y, output)\n",
    "\n",
    "confusion_test = confusion_matrix(curr_test_y, output_t)\n",
    "\n",
    "#Display confusion matrices, for report\n",
    "confusion_tr = pd.DataFrame(confusion_train, index = [i for i in ['Actual +', 'Actual -']],\n",
    "                  columns = [i for i in ['Pred +', 'Pred -']])\n",
    "\n",
    "confusion_te = pd.DataFrame(confusion_test, index = [i for i in ['Actual +', 'Actual -']],\n",
    "                  columns = [i for i in ['Pred +', 'Pred -']])\n",
    "\n",
    "#Training Plot\n",
    "fig, ax = plt.subplots()\n",
    "hm = sn.heatmap(confusion_tr, annot=True, fmt=\"d\", cmap='BuPu')\n",
    "hm.set_xticklabels(labels=hm.get_xticklabels(), va='center')\n",
    "hm.set_yticklabels(labels=hm.get_yticklabels(), va='center')\n",
    "ax.set_ylim([0,2])\n",
    "plt.title('Training Set Confusion Matrix', pad=30)\n",
    "plt.show()\n",
    "\n",
    "#Testing Plot\n",
    "fig, ax = plt.subplots()\n",
    "hm = sn.heatmap(confusion_te, annot=True, fmt=\"d\", cmap=\"OrRd\")\n",
    "hm.set_xticklabels(labels=hm.get_xticklabels(), va='center')\n",
    "hm.set_yticklabels(labels=hm.get_yticklabels(), va='center')\n",
    "ax.set_ylim([0,2])\n",
    "plt.title('Testing Set Confusion Matrix', pad=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Set Handling:\n",
    "curr_val_x = Xs[val]\n",
    "curr_val_y = ys[val]\n",
    "\n",
    "output_val = []\n",
    "\n",
    "v_losses = []\n",
    "\n",
    "for i, x in enumerate(curr_val_x):\n",
    "    x = Variable(torch.from_numpy(np.array(x).astype(np.float)))\n",
    "    pred_v, hc = model(x.float(), hprev, cprev)\n",
    "    y = Variable(torch.from_numpy(np.array(curr_val_y[l]).astype(np.int)))\n",
    "    y = y.unsqueeze(-1)\n",
    "    y = y.float()\n",
    "    #test loss\n",
    "    val_loss = loss_func(pred_v[-1], y)\n",
    "    v_losses.append(val_loss.item())\n",
    "    output_val.append(round(pred_v[-1].item()))\n",
    "\n",
    "correct_val = 0\n",
    "for i, y in enumerate(output_val):\n",
    "    if y == curr_val_y[i]:\n",
    "        correct_val = correct_val + 1\n",
    "val_acc = correct_val/len(output_val)\n",
    "\n",
    "print('Average Validation Loss is: ' + str(np.mean(v_losses)))\n",
    "print('Validation Accuracy is: ' + str(val_acc*100) + '%')\n",
    "\n",
    "confusion_val = confusion_matrix(curr_val_y, output_val)\n",
    "\n",
    "#Training Plot\n",
    "fig, ax = plt.subplots()\n",
    "hm = sn.heatmap(confusion_val, annot=True, fmt=\"d\", cmap='Pastel1')\n",
    "hm.set_xticklabels(labels=hm.get_xticklabels(), va='center')\n",
    "hm.set_yticklabels(labels=hm.get_yticklabels(), va='center')\n",
    "ax.set_ylim([0,2])\n",
    "plt.title('Validation Set Confusion Matrix', pad=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
